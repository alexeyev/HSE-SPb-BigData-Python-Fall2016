{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обзор средств для работы с распространёнными форматами данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Порой приходится работать с очень разными форматами данных.\n",
    "Если сталкиваемся с простыми и линейными -- нам помогут здравый смысл и стандартные простые средства."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Однако не стоит пренебрегать библиотеками и самостоятельно парсить CSV (например)!**\n",
    "Кавычки, переносы, разделители, кавычки внутри текста, комментарии, заголовки... \n",
    "\n",
    "Формат был установлен не сразу (https://tools.ietf.org/html/rfc4180.html).\n",
    "Дьявол в мелочах, масса вариантов формата. Всё это давно написано за нас с вами. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00e+00;1.00e-05;2.00e-05;3.00e-05;4.00e-05\n",
      "\n",
      "5.00e-05;6.00e-05;7.00e-05;8.00e-05;9.00e-05\n",
      "\n",
      "1.00e-04;1.10e-04;1.20e-04;1.30e-04;1.40e-04\n",
      "\n",
      "1.50e-04;1.60e-04;1.70e-04;1.80e-04;1.90e-04\n",
      "\n",
      "2.00e-04;2.10e-04;2.20e-04;2.30e-04;2.40e-04\n",
      "\n",
      "2.50e-04;2.60e-04;2.70e-04;2.80e-04;2.90e-04\n",
      "\n",
      "b\"\\x93NUMPY\\x01\\x00F\\x00{'descr': '<f8', 'fortran_order': False, 'shape': (10, 5), }         \\n\"\n",
      "b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xf1h\\xe3\\x88\\xb5\\xf8\\xe4>\\xf1h\\xe3\\x88\\xb5\\xf8\\xf4>i\\x1dUM\\x10u\\xff>\\xf1h\\xe3\\x88\\xb5\\xf8\\x04?-C\\x1c\\xeb\\xe26\\n'\n",
      "b'?i\\x1dUM\\x10u\\x0f?\\xd2\\xfb\\xc6\\xd7\\x9eY\\x12?\\xf1h\\xe3\\x88\\xb5\\xf8\\x14?\\x0f\\xd6\\xff9\\xcc\\x97\\x17?-C\\x1c\\xeb\\xe26\\x1a?K\\xb08\\x9c\\xf9\\xd5\\x1c?i\\x1dUM\\x10u\\x1f?C\\xc58\\x7f\\x13\\n'\n",
      "b'!?\\xd2\\xfb\\xc6\\xd7\\x9eY\"?a2U0*\\xa9#?\\xf1h\\xe3\\x88\\xb5\\xf8$?\\x80\\x9fq\\xe1@H&?\\x0f\\xd6\\xff9\\xcc\\x97\\'?\\x9e\\x0c\\x8e\\x92W\\xe7(?-C\\x1c\\xeb\\xe26*?\\xbcy\\xaaCn\\x86+?K\\xb08\\x9c\\xf9\\xd5,?\\xda\\xe6\\xc6\\xf4\\x84%.?i\\x1dUM\\x10u/?\\xfc\\xa9\\xf1\\xd2Mb0?C\\xc58\\x7f\\x13\\n'\n",
      "b\"1?\\x8b\\xe0\\x7f+\\xd9\\xb11?\\xd2\\xfb\\xc6\\xd7\\x9eY2?\\x1a\\x17\\x0e\\x84d\\x013?a2U0*\\xa93?\\xa9M\\x9c\\xdc\\xefP4?\\xf1h\\xe3\\x88\\xb5\\xf84?8\\x84*5{\\xa05?\\x80\\x9fq\\xe1@H6?\\xc7\\xba\\xb8\\x8d\\x06\\xf06?\\x0f\\xd6\\xff9\\xcc\\x977?V\\xf1F\\xe6\\x91?8?\\x9e\\x0c\\x8e\\x92W\\xe78?\\xe5'\\xd5>\\x1d\\x8f9?-C\\x1c\\xeb\\xe26:?t^c\\x97\\xa8\\xde:?\\xbcy\\xaaCn\\x86;?\\x03\\x95\\xf1\\xef3.<?K\\xb08\\x9c\\xf9\\xd5<?\\x92\\xcb\\x7fH\\xbf}=?\\xda\\xe6\\xc6\\xf4\\x84%>?!\\x02\\x0e\\xa1J\\xcd>?i\\x1dUM\\x10u??X\\x1c\\xce\\xfcj\\x0e@?\"\n"
     ]
    }
   ],
   "source": [
    "# Табличные форматы, которые можно читать невооружёнными глазами\n",
    "\n",
    "# Сохранение матрицы numpy с расширением txt\n",
    "# NB! Не получится сохранить многомерный массив!http://stackoverflow.com/a/3685339\n",
    "# numpy.savetxt(fname, X, fmt='%.18e', delimiter=' ', newline='\\n', header='', footer='', comments='# ')[source]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "synth_array = np.arange(50).reshape(10, 5) / 100000.0\n",
    "np.savetxt(\"my_text_representation.txt\", synth_array, fmt='%.2e', delimiter=';')\n",
    "\n",
    "for line_number, line in enumerate(open(\"my_text_representation.txt\", \"r\")):\n",
    "    print(line)\n",
    "    if line_number > 4:\n",
    "        break\n",
    "\n",
    "# в бинарном виде\n",
    "\n",
    "with open(\"my_text_representation.npy\", \"wb\") as output:        \n",
    "    np.save(output, synth_array)\n",
    "    \n",
    "for line_number, line in enumerate(open(\"my_text_representation.npy\", \"rb\")):\n",
    "    print(line)\n",
    "    if line_number > 4:\n",
    "        break\n",
    "\n",
    "## Самостоятельная работа\n",
    "# прочитать записанные в файлы матрицы средствами load и loadtxt\n",
    "\n",
    "## Самостоятельное изучение\n",
    "# cPickle -- способ сохранения и чтения произвольных объектов в бинарном виде; модуль в ст. библиотеке Python\n",
    "# используется очень часто"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_csv.reader object at 0x7f325856d748>\n",
      "['0.00e+00;1.00e-05;2.00e-05;3.00e-05;4.00e-05']\n",
      "['5.00e-05;6.00e-05;7.00e-05;8.00e-05;9.00e-05']\n",
      "['1.00e-04;1.10e-04;1.20e-04;1.30e-04;1.40e-04']\n",
      "['1.50e-04;1.60e-04;1.70e-04;1.80e-04;1.90e-04']\n",
      "['2.00e-04;2.10e-04;2.20e-04;2.30e-04;2.40e-04']\n",
      "['2.50e-04;2.60e-04;2.70e-04;2.80e-04;2.90e-04']\n",
      "['3.00e-04;3.10e-04;3.20e-04;3.30e-04;3.40e-04']\n",
      "['3.50e-04;3.60e-04;3.70e-04;3.80e-04;3.90e-04']\n",
      "['4.00e-04;4.10e-04;4.20e-04;4.30e-04;4.40e-04']\n",
      "['4.50e-04;4.60e-04;4.70e-04;4.80e-04;4.90e-04']\n",
      "   0.00e+00;1.00e-05;2.00e-05;3.00e-05;4.00e-05\n",
      "0  5.00e-05;6.00e-05;7.00e-05;8.00e-05;9.00e-05\n",
      "1  1.00e-04;1.10e-04;1.20e-04;1.30e-04;1.40e-04\n",
      "2  1.50e-04;1.60e-04;1.70e-04;1.80e-04;1.90e-04\n",
      "3  2.00e-04;2.10e-04;2.20e-04;2.30e-04;2.40e-04\n",
      "4  2.50e-04;2.60e-04;2.70e-04;2.80e-04;2.90e-04\n",
      "5  3.00e-04;3.10e-04;3.20e-04;3.30e-04;3.40e-04\n",
      "6  3.50e-04;3.60e-04;3.70e-04;3.80e-04;3.90e-04\n",
      "7  4.00e-04;4.10e-04;4.20e-04;4.30e-04;4.40e-04\n",
      "8  4.50e-04;4.60e-04;4.70e-04;4.80e-04;4.90e-04\n",
      "<class 'pandas.core.series.Series'>\n",
      "   0.00e+00;1.00e-05;2.00e-05;3.00e-05;4.00e-05\n",
      "0  5.00e-05;6.00e-05;7.00e-05;8.00e-05;9.00e-05\n",
      "2  1.50e-04;1.60e-04;1.70e-04;1.80e-04;1.90e-04\n"
     ]
    }
   ],
   "source": [
    "# Чтение-сохранение из csv/tsv средствами одноименной библиотеки\n",
    "\n",
    "import csv\n",
    "\n",
    "with open(\"my_text_representation.txt\", newline='\\n') as csvfile:\n",
    "    numslines = csv.reader(csvfile, delimiter=',')\n",
    "    print(numslines)\n",
    "    for row in numslines:\n",
    "        print(row)\n",
    "\n",
    "## Самостоятельная работа\n",
    "## Дописать: построить numpy.ndarray из того, что прочитано\n",
    "\n",
    "# Чтение-сохранение из csv/tsv средствами pandas\n",
    "# Практически то же самое, просто запись csv-шника в объект pandas.DataFrame\n",
    "# Кстати, посмотрите, сколько возможных параметров, это неспроста!\n",
    "# http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data_frame = pd.read_csv(\"my_text_representation.txt\")\n",
    "print(data_frame)\n",
    "\n",
    "# Всё ли с данными в порядке?\n",
    "\n",
    "# Размеры датасета\n",
    "# print(data_frame.shape)\n",
    "\n",
    "# Данные -- что с ними?\n",
    "# print(data_frame.ix[0])\n",
    "# print(type(data_frame.ix[0]))\n",
    "\n",
    "# по аналогии с numpy -- строки 0 и 2, все столбцы\n",
    "# print(data_frame.ix[[0, 2],:])\n",
    "\n",
    "## Самостоятельная работа\n",
    "## Разобраться с форматом, скачать и распаковать руками\n",
    "## http://opencorpora.org/files/export/ngrams/colloc.MI.zip\n",
    "## Оставить только текстовые колонки и сохранить в стандартном \n",
    "## формате с запятыми (google: pandas.to_csv) и добавив заголовки к каждой колонке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Element {test}a at 0x7fb16c0d5a88>\n",
      "b\"<?xml version='1.0' encoding='\"\n",
      "<Element {http://www.w3.org/2005/Atom}feed at 0x7fb16c068848>\n",
      "   <class 'lxml.etree._Element'>\n",
      "1980\n",
      "{http://www.w3.org/2005/Atom}feed\n",
      "{}\n",
      "[<Element {http://www.w3.org/2005/Atom}id at 0x7fb16c068688>, <Element {http://www.w3.org/2005/Atom}updated at 0x7fb16c0685c8>, <Element {http://www.w3.org/2005/Atom}category at 0x7fb16c068a48>, <Element {http://www.w3.org/2005/Atom}category at 0x7fb16c0689c8>, <Element {http://www.w3.org/2005/Atom}category at 0x7fb16c068908>]\n"
     ]
    }
   ],
   "source": [
    "# Не всегда данные так хорошо структурированы и не всегда имеют такую простую структуру\n",
    "# Примеры форматов с  \"иерархическим\" представлением данных: XML, JSON, etc.\n",
    "\n",
    "# XML, для работы с ним часто используют библиотеку lxml\n",
    "# for more, see: http://lxml.de/parsing.html and http://lxml.de/tutorial.html\n",
    "\n",
    "from lxml import etree\n",
    "from io import StringIO, BytesIO\n",
    "\n",
    "xml = '<a xmlns=\"test\"><b xmlns=\"test\"/></a>'\n",
    "root = etree.fromstring(xml)\n",
    "print(root)\n",
    "\n",
    "## Возьмём какой-нибудь фид из сети\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "with urllib.request.urlopen(\"http://jazzandblues.blogspot.com/feeds/posts/default\") as response:\n",
    "    charset = response.info().get_content_charset()\n",
    "    xml = response.read()\n",
    "    print(xml[:30])\n",
    "\n",
    "# распечатаем то, с чем имеем дело\n",
    "# print(etree.tostring(root))\n",
    "\n",
    "# зададим парсер\n",
    "parser = etree.XMLParser(ns_clean=True, remove_blank_text=True)\n",
    "\n",
    "# разберём дерево\n",
    "tree   = etree.parse(BytesIO(xml), parser)\n",
    "\n",
    "# print(etree.tostring(tree))\n",
    "\n",
    "for element in tree.iter():\n",
    "    print(element)\n",
    "    print(\"  \", type(element))\n",
    "    print(len(element))\n",
    "    print(element.tag)\n",
    "    print(element.attrib)\n",
    "    print(list(element)[:5])\n",
    "    break\n",
    "\n",
    "## Самостоятельная работа\n",
    "# Написать функцию, которая переводит правильную скобочную последовательность \n",
    "# в соответствующий XML с одним и тем же тегом. Нужно использовать Element, append и т.д.\n",
    "# См. tutorial: http://lxml.de/tutorial.html\n",
    "\n",
    "# Например, (()()) -> <mytag><mytag></mytag><mytag></mytag></mytag>\n",
    "\n",
    "## Самостоятельное изучение \n",
    "# Как читать из файла? Как записать в файл?\n",
    "\n",
    "## Самостоятельное изучение\n",
    "# Язык запросов к XML-документам -- xpath\n",
    "# http://stackoverflow.com/questions/8692/how-to-use-xpath-in-python\n",
    "# http://www.w3schools.com/xml/xpath_examples.asp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"foo\", {\"bar\":\n",
      "['baz', None, 1.0, 2]\n",
      "{\"show\": {\"way\": [\"to\", \"the\", {\"next\": [\"show\", 12, 13.0, true, null]}], \"me\": \"the\"}}\n"
     ]
    }
   ],
   "source": [
    "# Просто запишем JSON в словарь и наоборот\n",
    "# Tutorial: https://docs.python.org/3/library/json.html\n",
    "\n",
    "import json\n",
    "\n",
    "# dumps -- создаёт строку!\n",
    "example = json.dumps(['foo', {'bar': ('baz', None, 1.0, 2)}])\n",
    "print(example[:15])\n",
    "\n",
    "# loads -- читает из строки, по сути парсит JSON\n",
    "print(json.loads(example)[1][\"bar\"])\n",
    "\n",
    "# dump -- пишет в файл\n",
    "with open('data.json', 'w') as outfile:\n",
    "    json.dump({\"show\" : {\"me\" : \"the\", \"way\" : [\"to\", \"the\", {\"next\" : [\"show\" ,12, 13.0, True, None]}]}}, outfile)\n",
    "\n",
    "print(open(\"data.json\", \"r\").read())\n",
    "\n",
    "## Самостоятельная работа\n",
    "# Применить load (чтение из файла data.json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 29 elements.\n",
      "There were 0 attributes.\n",
      "---ELEMENT TYPES\n",
      "               PRICE 4\n",
      "               TITLE 4\n",
      "                YEAR 4\n",
      "             CATALOG 1\n",
      "             COMPANY 4\n",
      "             COUNTRY 4\n",
      "                  CD 4\n",
      "              ARTIST 4\n",
      "---ATTRIBUTE TYPES\n"
     ]
    }
   ],
   "source": [
    "# Не всегда такие данные влезают в память -- для этого есть т.н. потоковые парсеры -- они читают файл как поток и имеют\n",
    "# ограничения по используемой памяти; заходя в определённую нами же \"ветку\", мы можем выполнить какое-то действие с\n",
    "# помощью так называемой callback-функции\n",
    "\n",
    "# SAX (Simple API for XML) is an event-driven online algorithm for parsing XML documents,\n",
    "# with an API developed by the XML-DEV mailing list.\n",
    "\n",
    "from xml.sax import make_parser, handler\n",
    "\n",
    "data = \"\"\"\n",
    "<CATALOG>\n",
    "<CD>\n",
    "<TITLE>Empire Burlesque</TITLE>\n",
    "<ARTIST>Bob Dylan</ARTIST>\n",
    "<COUNTRY>USA</COUNTRY>\n",
    "<COMPANY>Columbia</COMPANY>\n",
    "<PRICE>10.90</PRICE>\n",
    "<YEAR>1985</YEAR>\n",
    "</CD>\n",
    "<CD>\n",
    "<TITLE>Hide your heart</TITLE>\n",
    "<ARTIST>Bonnie Tyler</ARTIST>\n",
    "<COUNTRY>UK</COUNTRY>\n",
    "<COMPANY>CBS Records</COMPANY>\n",
    "<PRICE>9.90</PRICE>\n",
    "<YEAR>1988</YEAR>\n",
    "</CD>\n",
    "<CD>\n",
    "<TITLE>Greatest Hits</TITLE>\n",
    "<ARTIST>Dolly Parton</ARTIST>\n",
    "<COUNTRY>USA</COUNTRY>\n",
    "<COMPANY>RCA</COMPANY>\n",
    "<PRICE>9.90</PRICE>\n",
    "<YEAR>1982</YEAR>\n",
    "</CD>\n",
    "<CD>\n",
    "<TITLE>Still got the blues</TITLE>\n",
    "<ARTIST>Gary Moore</ARTIST>\n",
    "<COUNTRY>UK</COUNTRY>\n",
    "<COMPANY>Virgin records</COMPANY>\n",
    "<PRICE>10.20</PRICE>\n",
    "<YEAR>1990</YEAR>\n",
    "</CD>\n",
    "</CATALOG>\n",
    "\"\"\"\n",
    "\n",
    "with open(\"huge_data.xml\", \"w\") as outfile:\n",
    "    outfile.write(data)\n",
    "\n",
    "class MyLovelyCounter(handler.ContentHandler):\n",
    "\n",
    "    def __init__(self):\n",
    "        self._elems = 0\n",
    "        self._attrs = 0\n",
    "        self._elem_types = {}\n",
    "        self._attr_types = {}\n",
    "\n",
    "    def startElement(self, name, attrs):\n",
    "        \n",
    "        # print(\"Hello\", name)\n",
    "        \n",
    "        self._elems = self._elems + 1\n",
    "        self._attrs = self._attrs + len(attrs)\n",
    "        self._elem_types[name] = self._elem_types.get(name, 0) + 1\n",
    "\n",
    "        for name in attrs.keys():\n",
    "            self._attr_types[name] = self._attr_types.get(name, 0) + 1\n",
    "        \n",
    "    def endElement(self, name):\n",
    "        pass\n",
    "        # print(\"Goodbye\", name)\n",
    "\n",
    "    def endDocument(self):\n",
    "        \n",
    "        print(\"There were\", self._elems, \"elements.\")\n",
    "        print(\"There were\", self._attrs, \"attributes.\")\n",
    "        print(\"---ELEMENT TYPES\")\n",
    "        \n",
    "        for pair in  self._elem_types.items():\n",
    "            print(\"%20s %d\" % pair)\n",
    "\n",
    "        print(\"---ATTRIBUTE TYPES\")\n",
    "        \n",
    "        for pair in  self._attr_types.items():\n",
    "            print(\"%20s %d\" % pair)\n",
    "\n",
    "            \n",
    "parser = make_parser()\n",
    "parser.setContentHandler(MyLovelyCounter())\n",
    "parser.parse(open(\"huge_data.xml\",\"r\"))\n",
    "\n",
    "## Самостоятельное изучение\n",
    "# Аналогичные средства есть и для JSON\n",
    "# Хорошие примеры есть на главной странице документации ijson: https://pypi.python.org/pypi/ijson/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>Друзь Александр Абрамович - Спортивное \"Что? Где? Когда?\" Официальный рейтинг МАК</title>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/bs4/__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "# HTML -- это \"грязный\" XML (есть и \"чистая\" модификация -- XHTML), всем известный язык разметки веб-страниц.\n",
    "# Чтобы что-то вытащить, рекомендуется использовать специализированные средства.\n",
    "# Одна из самых известных библиотек разбора HTML -- BeautifulSoup.\n",
    "\n",
    "# самый простой способ скачать страницу\n",
    "import urllib.request\n",
    "\n",
    "# будем вытаскивать данные из \"плохой\" страницы\n",
    "response = urllib.request.urlopen(\"http://rating.chgk.info/player/9808\")\n",
    "html = response.read()\n",
    "\n",
    "# посмотрите на нужный элемент в браузере с помощью inspect\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(html)\n",
    "\n",
    "print(soup.html.head.title)\n",
    "# print(soup.html.head.name) # имя тега\n",
    "# print(soup.html.head.text) # текст\n",
    "# print(soup.html.head.attrs) # атрибуты, словарь\n",
    "# print(list(soup.html.head.children)) # наследники\n",
    "\n",
    "matches = soup.findAll(\"table\", {\"class\":\"tournaments_table\"})\n",
    "\n",
    "for table in matches:\n",
    "    \n",
    "    # все строки таблицы\n",
    "    rows = table.findAll(\"tr\")\n",
    "    \n",
    "    for row in rows:    \n",
    "        # все строки таблицы, у которых не указан класс\n",
    "        if \"class\" in row.attrs:\n",
    "            pass\n",
    "            # print(row.attrs[\"class\"])\n",
    "        else:\n",
    "            # pass\n",
    "            columns = row.findAll(\"td\")\n",
    "            if len(columns) > 1:\n",
    "                pass\n",
    "                # print(columns[2].text.strip())\n",
    "\n",
    "# 1. Но всё же здесь лучше задать xpath и всё им вытащить.\n",
    "# 2. Для подобного сбора данных лучше использовать специализированные средства -- use Scrapy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Задачи\n",
    "Текст курсивом (или в \"звёздочках\", если клетка в режиме редактирования)-- полезная практика; не всегда и не всё было рассказано на занятии -- и не всё будет. \n",
    "Делать в классе, если есть другие \"стандартные задачи\" -- не надо.\n",
    "\n",
    "0. Скачать opencorpora, версию со снятой омонимией (см. http://opencorpora.org/?page=downloads). Вытащить из XML все предложения и записать в файл как единый текст, разбивая на абзацы и проставляя заголовки. // *Далее -- \"со звёздочкой\" и \"на потом\": \"подогнать\" с помощью регулярных выражений функцию разбиения текста на предложения. Придумать оценку качества разбиения. Проверить на любом новостном большом тексте -- по вашей оценке и \"на глаз\".*\n",
    "    \n",
    "    <text id=\"2\" parent=\"1\" name=\"00021 Школа злословия\">\n",
    "      <tags>\n",
    "        <tag>Год:2008</tag>\n",
    "        <tag>Дата:25/08</tag>\n",
    "        <tag>Автор:Валентин Колесников</tag>\n",
    "        <tag>url:http://www.chaskor.ru/article/shkola_zlosloviya_uchit_prikusit_yazyk_21</tag>\n",
    "        <tag>Тема:ЧасКор:Медиа</tag>\n",
    "        <tag>Тема:ЧасКор:Медиа/ТВ и радио</tag>\n",
    "      </tags>\n",
    "      <paragraphs>\n",
    "        <paragraph id=\"1\">\n",
    "          <sentence id=\"1\">\n",
    "            <source>«Школа злословия» учит прикусить язык</source>\n",
    "    \n",
    "\n",
    "1. Написать конвертер из XML в JSON. Можно считать, что всё влезет в память. *А если нет?*\n",
    "\n",
    "2. Выкачать фиды Яндекс.Новостей для нескольких категорий (e.g. категория Бизнес: https://news.yandex.ru/business.rss) с помощью urllib и lxml и записать их в файл с помощью pandas в выбранном  вами \"плоском\" формате, отдельной колонкой добавив категорию. *Используя TfIdfVectorizer и LogisticRegression из sklearn написать классификатор. Перемешать данные, разделить на две части. Обучить на одной, проверить точность классификации на другой. После рассмотреть значимость фич.*\n",
    "\n",
    "3. *Выбрать заведения, к которым бывает нужен скорый доступ. Или просто важные \"места\" в городе, у которых одна и та же функция. Найти список с адресами в сети (пример: http://med-info.ru/reference/list/3/1#r и отдельное заведение -- http://med-info.ru/reference/view/173). С помощью urllib2 -- скачать нужные html и записать в строку; при необходимости можно записать на диск. С помощью BeauifulSoup -- разобрать html и извлечь адреса. С помощью urllib2 обратиться в Я.Геокодер с запросами-адресами https://tech.yandex.ru/maps/doc/geocoder/desc/concepts/response_structure-docpage/#json_response. С помощью библиотеки json извлечь координаты наших заведений. С помощью matplotlib визуализировать расположение точек на \"плоскости\" (на карту накладывать необязательно). Подумать, как можно отобразить \"ещё нагляднее\".*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Что ещё бывает?\n",
    " - XLS, XLSX \n",
    " - SQL dumps\n",
    " - ARFF, SPSS (редко IRL)\n",
    " - protobuf (но всё же это для передачи данных, а не для их распространения)\n",
    " - domain-dependent data markup: CoNLL, VoiceXML, MusicXML, ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}