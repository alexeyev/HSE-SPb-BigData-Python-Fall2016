{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Нежное введение в ту часть машинного обучения\n",
    "#### задачи из которой решает sklearn\n",
    "* Supervised learning: classification/regression\n",
    "* Unsupervised learning (for structure discovery and visualization): clustering, dimensionality reduction, etc.\n",
    "\n",
    "#### Задачи, которые обычно относят к машинному обучению -- в одной куче (wiki):\n",
    "* **Classification **\n",
    "* **Clustering **\n",
    "* **Regression **\n",
    "* Anomaly detection \n",
    "* Association rules \n",
    "* Reinforcement learning \n",
    "* Structured prediction \n",
    "* **Feature engineering **\n",
    "* Feature learning \n",
    "* Online learning \n",
    "* **Semi-supervised learning** \n",
    "* **Unsupervised learning** \n",
    "* Learning to rank \n",
    "* Grammar induction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Настоятельно рекомендую ознакомиться:\n",
    "https://www.coursera.org/learn/vvedenie-mashinnoe-obuchenie/home/week/7\n",
    "А лучше вообще пройти весь курс.\n",
    "\n",
    "Нужно понять\n",
    "1. какую задачу мы решаем;\n",
    "2. как мы будем оценивать свои успехи (может, нам вовсе и не нужен машин лёрнинг?);\n",
    "3. какими данными мы располагаем;\n",
    "4. как именно по данным будем строить фичи (+ фичи на основе фич);\n",
    "5. как отмасштабировать-отфильтровать данные, если в них есть мусор;\n",
    "6. с каких моделей стоит начать;\n",
    "7. как оценивать качество модели (в т.ч. случайно ли выбирать holdout)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4) (150,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ -1.69031455e-15,  -1.63702385e-15,  -1.48251781e-15,\n",
       "         -1.62314606e-15]), array([ 1.,  1.,  1.,  1.]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    " Самостоятельная работа.\n",
    " \n",
    " https://ru.wikipedia.org/wiki/%D0%98%D1%80%D0%B8%D1%81%D1%8B_%D0%A4%D0%B8%D1%88%D0%B5%D1%80%D0%B0\n",
    " \n",
    " Исследовать классический датасет. \n",
    " \n",
    " - Что за фичи? Как распределены из значения?\n",
    " \n",
    " - Сколько классов? Хорошо ли они сбалансированы?\n",
    " \n",
    " - Проверить несколько известных вам моделей для классификации методом отложенной выборки; \n",
    "   accuracy, точность, полнота, f-мера. \n",
    "   Рекомендуется попробовать как минимум KNN, LogisticRegression, DecisionTree.\n",
    "   \n",
    " - Попробуйте применить какую-нибудь линейную модель для классификации. \n",
    "   Нормализуйте фичи с помощью StandardScaler или как предложено ниже.\n",
    "   Снова примените линейную модель. Есть выгода?\n",
    " \n",
    " - Поменять их параметры руками; как меняется качество? есть идеи, почему?\n",
    " \n",
    " - Выбрать любимую модель, подобрать её параметры с помощью GridSearchCV (см. примеры в большом ДЗ)\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "print(iris.data.shape, iris.target.shape)\n",
    "\n",
    "# hint\n",
    "unique, counts = np.unique(iris.target, return_counts=True)\n",
    "\n",
    "# hint\n",
    "X_scaled = preprocessing.scale(iris.data)\n",
    "\n",
    "# что это значит?\n",
    "np.mean(X_scaled, axis=0), np.std(X_scaled, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import cluster, datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target\n",
    "\n",
    "k_means = cluster.KMeans(n_clusters=3)\n",
    "k_means.fit(X_iris) \n",
    "\n",
    "k_means.labels_[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Так уж получилось, что ирисы неплохо \"кластеризованы\", \n",
    "# поэтому метод ближайших соседей неплохо работал на классификации.\n",
    "\n",
    "# Note: in real life, мы почти никогда не знаем ни полного \"правильного разбиения\", ни смысла кластеров.\n",
    "# Поэтому нужны другие методы оценки качества кластеризации.\n",
    "\n",
    "# Посмотрим, насколько хорошо обучилось отображение класс -> кластер\n",
    "\n",
    "import pandas as pd # overkill, but still\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df[\"true\"] = iris.target\n",
    "df[\"cluster\"] = k_means.labels_\n",
    "\n",
    "# print(df.groupby([\"true\", \"cluster\"]).size())\n",
    "\n",
    "# видим, что один класс разбивается на два кластера \n",
    "# было бы здорово взглянуть на это всё на картинке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfig = plt.figure(1, figsize=(8, 6))\\nax = Axes3D(fig, elev=-150, azim=110)\\nX_reduced = PCA(n_components=2).fit_transform(iris.data)\\n\\nax.scatter(X_reduced[:, 0], X_reduced[:, 1], X_reduced[:, 2], c=Y, cmap=plt.cm.Paired)\\nax.set_title(\"First three PCA directions\")\\nax.set_xlabel(\"1st eigenvector\")\\nax.w_xaxis.set_ticklabels([])\\nax.set_ylabel(\"2nd eigenvector\")\\nax.w_yaxis.set_ticklabels([])\\nax.set_zlabel(\"3rd eigenvector\")\\nax.w_zaxis.set_ticklabels([])\\nplt.show()\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0xb4e0c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Adapted from\n",
    "# http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "# from sklearn.decomposition import PCA\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "features = [\"sepal length\", \"sepal width\", \"petal length\", \"petal width\"]\n",
    "\n",
    "X = iris.data\n",
    "Y = iris.target\n",
    "\n",
    "plt.figure(1) \n",
    "plt.clf()\n",
    "\n",
    "# Отрисовываем по двум фичам\n",
    "# for f0 in range(4):\n",
    "#     for f1 in range(f0):\n",
    "#         plt.scatter(X[:, f0], X[:, f1], c=Y * Y, cmap=plt.cm.Paired)\n",
    "#         plt.xticks(())\n",
    "#         plt.yticks(())\n",
    "#         plt.xlabel(features[f0])\n",
    "#         plt.ylabel(features[f1])\n",
    "#         plt.show()\n",
    "\n",
    "\n",
    "# Можно и в трёх измерениях\n",
    "# Самостоятельное изучение\n",
    "\"\"\"\n",
    "fig = plt.figure(1, figsize=(8, 6))\n",
    "ax = Axes3D(fig, elev=-150, azim=110)\n",
    "X_reduced = PCA(n_components=2).fit_transform(iris.data)\n",
    "\n",
    "ax.scatter(X_reduced[:, 0], X_reduced[:, 1], X_reduced[:, 2], c=Y, cmap=plt.cm.Paired)\n",
    "ax.set_title(\"First three PCA directions\")\n",
    "ax.set_xlabel(\"1st eigenvector\")\n",
    "ax.w_xaxis.set_ticklabels([])\n",
    "ax.set_ylabel(\"2nd eigenvector\")\n",
    "ax.w_yaxis.set_ticklabels([])\n",
    "ax.set_zlabel(\"3rd eigenvector\")\n",
    "ax.w_zaxis.set_ticklabels([])\n",
    "plt.show()\n",
    "\"\"\"\n",
    "\n",
    "# ...и тут мы понимаем, что линейному классификатору действительно должно быть оооочень просто!\n",
    "# Но кластеризатор \"думает сам\", двумерные проекции хорошо показывают деление на два кластера,\n",
    "# впрочем, они действительно могут рассказать не всё. \n",
    "\n",
    "# Самостоятельная работа\n",
    "# 0) *Посмотреть то же в 3D.\n",
    "# 1) Поменять метки класса на метки кластера и сформулировать, как именно \"глупит\" KMeans.\n",
    "# 2) Применить другой метод кластеризации для малых размерностей из sklearn; посмотреть на разные варианты \n",
    "#    расстояний между объектами.\n",
    "# 3) *Неплохие инструменты есть и в scipy: https://docs.scipy.org/doc/scipy-0.18.1/reference/cluster.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Самостоятельная работа\n",
    "\n",
    "Загрузите, кластеризуйте и визуализируйте (google the docs) разбиение датасетов несколькими алгоритмами: \n",
    "BIRCH, DBSCAN, agglomerative clustering, KMeans (MiniBatchKMeans), %your_favourive_clustering_algorithm%\n",
    "\n",
    "Сделайте выводы об их ограничениях и преимуществах.\n",
    "\n",
    "\"\"\"\n",
    "from sklearn import cluster, datasets\n",
    "import numpy as np\n",
    "\n",
    "n_items = 2000\n",
    "\n",
    "circles = datasets.make_circles(n_samples=n_items, factor=.5, noise=.02)\n",
    "moons = datasets.make_moons(n_samples=n_items, noise=.03)\n",
    "blobs = datasets.make_blobs(n_samples=n_items, random_state=8)\n",
    "totally_random = np.random.rand(n_items, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Самостоятельная работа\n",
    "\n",
    "Возьмите данные из Большой Домашней Работы и примените к ним алгоритм иерархической кластеризации из\n",
    "sklearn или из scipy (помните, что для некоторых расстояний векторы стоит нормализовать!).\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering\n",
    "https://docs.scipy.org/doc/scipy-0.18.1/reference/cluster.hierarchy.html\n",
    "\n",
    "Посмотрите, что происходит при изменении числа кластеров?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Домашняя работа*\n",
    "\n",
    "http://ling.go.mail.ru/dsm/en/about#models\n",
    "Скачайте любую из моделей (рекомендую наименьшую).\n",
    "\n",
    "Векторов много, поэтому есть смысл отфильтровать слова по \"тегам\" -- то есть оставить \n",
    "какую-то одну часть речи.\n",
    "\n",
    "Векторов всё ещё много (наверняка), поэтому придётся взять случайное подмножество. \n",
    "Однако если памяти на следующие шаги хватит, то можно оставить и так.\n",
    "\n",
    "Разбейте на кластеры (едва ли тут что-то будет работать за разумное время, кроме MiniBatchKMeans): \n",
    "n = 100, 200, 500, 1000, 1500. \n",
    "distance = косинусное, евклидово\n",
    "\n",
    "Распечатайте кластеры слов.\n",
    "\n",
    "Получилось что-то осмысленное, верно? :)\n",
    "Какое разбиение вам больше нравится?\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
